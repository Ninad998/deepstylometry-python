{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# coding: utf-8\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "np.random.seed(123)\n",
    "\n",
    "import pandas as pd\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.layers import Dense, Flatten\n",
    "from keras.layers import Convolution1D, MaxPooling1D, Embedding\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dropout\n",
    "from keras.optimizers import SGD, Adadelta\n",
    "from keras.models import Sequential\n",
    "import sys\n",
    "\n",
    "BASE_DIR = '../../'\n",
    "GLOVE_DIR = BASE_DIR + 'glove/'\n",
    "\n",
    "\n",
    "MAX_NB_WORDS = 20000\n",
    "VALIDATION_SPLIT = 0.2\n",
    "CONVOLUTION_FEATURE = 256\n",
    "BORDER_MODE = 'valid'\n",
    "DENSE_FEATURE = 1024\n",
    "DROP_OUT = 0.4\n",
    "LEARNING_RATE=0.0001\n",
    "MOMENTUM=0.9\n",
    "EPOCH=25\n",
    "BATCH_SIZE=16\n",
    "EMBEDDING_DIM = 200\n",
    "embedfile = 'glove.6B.' + str(EMBEDDING_DIM) + 'd.txt'\n",
    "doc_id = 161\n",
    "author_id = 80\n",
    "authorList = [11, 18, 80, 88, 64]\n",
    "chunk_size = 1500\n",
    "MAX_SEQUENCE_LENGTH = chunk_size\n",
    "nb_epoch = 30\n",
    "EPOCH = nb_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# first, build index mapping words in the embeddings set\n",
    "# to their embedding vector\n",
    "\n",
    "print('Indexing word vectors.')\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join(GLOVE_DIR, embedfile))\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "# second, prepare text samples and their labels\n",
    "print('Processing text dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "texts = []  # list of text samples\n",
    "labels_index = {}  # dictionary mapping label name to numeric id\n",
    "labels = []  # list of label ids\n",
    "import DatabaseQuery\n",
    "# textToUse = pd.read_csv(\"suffle_4_6000.csv\", names=[\"author_id\", \"doc_content\"], dtype={'author_id': int})\n",
    "from sshtunnel import SSHTunnelForwarder\n",
    "PORT=5432\n",
    "with SSHTunnelForwarder(('srn02.cs.cityu.edu.hk', 22),\n",
    "                        ssh_username='stylometry',\n",
    "                        ssh_password='stylometry',\n",
    "                        remote_bind_address=('localhost', 5432),\n",
    "                        local_bind_address=('localhost', 5400)):\n",
    "    textToUse = DatabaseQuery.getWordAuthData(5400, authors = authorList, doc = doc_id,\n",
    "                                              chunk_size = chunk_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "textToUse.loc[1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labels = []\n",
    "texts = []\n",
    "size = []\n",
    "authorList = textToUse.author_id.unique()\n",
    "for auth in authorList:\n",
    "    current = textToUse.loc[textToUse['author_id'] == auth]\n",
    "    size.append(current.shape[0])\n",
    "    print(\"Author: %5s  Size: %5s\" % (auth, current.shape[0]))\n",
    "print(\"Min: %s\" % (min(size)))\n",
    "print(\"Max: %s\" % (max(size)))\n",
    "\n",
    "authorList = authorList.tolist()\n",
    "\n",
    "for auth in authorList:\n",
    "    current = textToUse.loc[textToUse['author_id'] == auth]\n",
    "    samples = min(size)\n",
    "    current = current.sample(n = samples)\n",
    "    textlist = current.doc_content.tolist()\n",
    "    texts = texts + textlist\n",
    "    labels = labels + [authorList.index(author_id) for author_id in current.author_id.tolist()]\n",
    "\n",
    "labels_index = {}\n",
    "labels_index[0] = 0\n",
    "for i, auth in enumerate(authorList):\n",
    "    labels_index[i] = auth\n",
    "\n",
    "del textToUse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Found %s texts.' % len(texts))\n",
    "print('Found %s labels.' % len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(labels[10])\n",
    "print(texts[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# finally, vectorize the text samples into a 2D integer tensor\n",
    "tokenizer = Tokenizer(nb_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels = to_categorical(np.asarray(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labels[10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data[10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# split the data into a training set and a validation set\n",
    "# from sklearn.model_selection import KFold\n",
    "# kfold = KFold(n_splits=6, shuffle=True, random_state=123)\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_val, y_train, y_val = train_test_split(data, labels, test_size=VALIDATION_SPLIT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Preparing embedding matrix.')\n",
    "\n",
    "# prepare embedding matrix\n",
    "nb_words = len(word_index)\n",
    "embedding_matrix = np.zeros((nb_words + 1, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if i > MAX_NB_WORDS:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "del embeddings_index\n",
    "\n",
    "print('Training model.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(nb_words)\n",
    "print(MAX_NB_WORDS)\n",
    "print(len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# def getModel():\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(                          # Layer 0, Start\n",
    "    input_dim=nb_words + 1,                   # Size to dictionary, has to be input + 1\n",
    "    output_dim=EMBEDDING_DIM,                 # Dimensions to generate\n",
    "    weights=[embedding_matrix],               # Initialize word weights\n",
    "    input_length=MAX_SEQUENCE_LENGTH,         # Define length to input sequences in the first layer\n",
    "    trainable=False))                         # Disable weight changes during training\n",
    "\n",
    "model.add(Convolution1D(                      # Layer 1,   Features: 256, Kernel Size: 7\n",
    "    nb_filter=CONVOLUTION_FEATURE,            # Number of kernels or number of filters to generate\n",
    "    filter_length=7,                          # Size of kernels\n",
    "    border_mode=BORDER_MODE,                  # Border = 'valid', cause kernel to reduce dimensions\n",
    "    activation='relu'))                       # Activation function to use\n",
    "\n",
    "model.add(MaxPooling1D(                       # Layer 1a,  Max Pooling: 3\n",
    "    pool_length=3))                           # Size of kernels\n",
    "\n",
    "model.add(Convolution1D(                      # Layer 2,   Features: 256, Kernel Size: 7\n",
    "    nb_filter=CONVOLUTION_FEATURE,            # Number of kernels or number of filters to generate\n",
    "    filter_length=7,                          # Size of kernels\n",
    "    border_mode=BORDER_MODE,                  # Border = 'valid', cause kernel to reduce dimensions\n",
    "    activation='relu'))                       # Activation function to use\n",
    "\n",
    "model.add(MaxPooling1D(                       # Layer 2a,  Max Pooling: 3\n",
    "    pool_length=3))                           # Size of kernels\n",
    "\n",
    "model.add(Convolution1D(                      # Layer 3,   Features: 256, Kernel Size: 3\n",
    "    nb_filter=CONVOLUTION_FEATURE,            # Number of kernels or number of filters to generate\n",
    "    filter_length=3,                          # Size of kernels\n",
    "    border_mode=BORDER_MODE,                  # Border = 'valid', cause kernel to reduce dimensions\n",
    "    activation='relu'))                       # Activation function to use\n",
    "\n",
    "model.add(Convolution1D(                      # Layer 4,   Features: 256, Kernel Size: 3\n",
    "    nb_filter=CONVOLUTION_FEATURE,            # Number of kernels or number of filters to generate\n",
    "    filter_length=3,                          # Size of kernels\n",
    "    border_mode=BORDER_MODE,                  # Border = 'valid', cause kernel to reduce dimensions\n",
    "    activation='relu'))                       # Activation function to use\n",
    "\n",
    "model.add(Convolution1D(                      # Layer 5,   Features: 256, Kernel Size: 3\n",
    "    nb_filter=CONVOLUTION_FEATURE,            # Number of kernels or number of filters to generate\n",
    "    filter_length=3,                          # Size of kernels\n",
    "    border_mode=BORDER_MODE,                  # Border = 'valid', cause kernel to reduce dimensions\n",
    "    activation='relu'))                       # Activation function to use\n",
    "\n",
    "model.add(Convolution1D(                      # Layer 6,   Features: 256, Kernel Size: 3\n",
    "    nb_filter=CONVOLUTION_FEATURE,            # Number of kernels or number of filters to generate\n",
    "    filter_length=3,                          # Size of kernels\n",
    "    border_mode=BORDER_MODE,                  # Border = 'valid', cause kernel to reduce dimensions\n",
    "    activation='relu'))                       # Activation function to use\n",
    "\n",
    "model.add(MaxPooling1D(                       # Layer 6a,  Max Pooling: 3\n",
    "    pool_length=3))                           # Size of kernels\n",
    "\n",
    "model.add(Flatten())                          # Layer 7\n",
    "\n",
    "model.add(Dense(                              # Layer 7a,  Output Size: 1024\n",
    "    output_dim=DENSE_FEATURE,                 # Output dimension\n",
    "    activation='relu'))                       # Activation function to use\n",
    "\n",
    "model.add(Dropout(DROP_OUT))\n",
    "\n",
    "model.add(Dense(                              # Layer 8,   Output Size: 1024\n",
    "    output_dim=DENSE_FEATURE,                 # Output dimension\n",
    "    activation='relu'))                       # Activation function to use\n",
    "\n",
    "model.add(Dropout(DROP_OUT))\n",
    "\n",
    "model.add(Dense(                              # Layer 9,  Output Size: Size Unique Labels, Final\n",
    "    output_dim=len(labels_index),             # Output dimension\n",
    "    activation='softmax'))                    # Activation function to use\n",
    "\n",
    "# model = Model(start, end)\n",
    "\n",
    "sgd = SGD(lr=LEARNING_RATE, momentum=MOMENTUM, nesterov=True)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print(\"Done compiling.\")\n",
    "    \n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# trainindices = []\n",
    "# testindices = []\n",
    "# for train, test in kfold.split(data):\n",
    "#     print(\"train %s\" % (str(train)))\n",
    "#     print(\"test %s\" % (str(test)))\n",
    "#     trainindices.append(train)\n",
    "#     testindices.append(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cvscores = []\n",
    "\n",
    "# x_train, x_val, y_train, y_val = train_test_split(data, labels, test_size=VALIDATION_SPLIT)\n",
    "\n",
    "# x_train = data[trainindices[0]]\n",
    "# y_train = labels[trainindices[0]]\n",
    "# x_val = data[testindices[0]]\n",
    "# y_val = labels[testindices[0]]\n",
    "# model = getModel()\n",
    "history = model.fit(x_train, y_train, validation_data=(x_val, y_val),\n",
    "                    nb_epoch=EPOCH, batch_size=BATCH_SIZE)\n",
    "scores = model.evaluate(x_val, y_val, verbose=0)\n",
    "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "del model, x_train, y_train, x_val, y_val\n",
    "cvscores.append(scores[1] * 100)\n",
    "\n",
    "import time\n",
    "time.sleep(60)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"%.2f%% (+/- %.2f%%)\" % (numpy.mean(cvscores), numpy.std(cvscores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "texts = []  # list of text samples\n",
    "labels = []  # list of label ids\n",
    "import DatabaseQuery\n",
    "# textToUse = pd.read_csv(\"suffle_4_6000.csv\", names=[\"author_id\", \"doc_content\"], dtype={'author_id': int})\n",
    "from sshtunnel import SSHTunnelForwarder\n",
    "PORT=5432\n",
    "with SSHTunnelForwarder(('srn02.cs.cityu.edu.hk', 22),\n",
    "                        ssh_username='stylometry',\n",
    "                        ssh_password='stylometry',\n",
    "                        remote_bind_address=('localhost', 5432),\n",
    "                        local_bind_address=('localhost', 5400)):\n",
    "    textToUse = DatabaseQuery.getWordDocData(5400, doc_id, chunk_size = chunk_size)\n",
    "labels = []\n",
    "texts = []\n",
    "for index, row in textToUse.iterrows():\n",
    "    labels.append(authorList.index(row.author_id))\n",
    "    texts.append(row.doc_content)\n",
    "        \n",
    "print('Found %s texts.' % len(texts))\n",
    "\n",
    "del textToUse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# finally, vectorize the text samples into a 2D integer tensor\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "X = pad_sequences(sequences, maxlen = chunk_size)\n",
    "\n",
    "print('Shape of data tensor:', X.shape)\n",
    "\n",
    "testX = X[:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to take input of data and return prediction model\n",
    "predY = np.array(model.predict(testX, batch_size=BATCH_SIZE))\n",
    "def entroPred(predY):\n",
    "    predYList = predY[:]\n",
    "    entro = []\n",
    "    import math\n",
    "    for row in predY:\n",
    "        entroval = 0\n",
    "        for i in row:\n",
    "            entroval += (i * (math.log(i , 2)))\n",
    "        entroval = -1 * entroval\n",
    "        entro.append(entroval)\n",
    "    yx = zip(entro, predY)\n",
    "    yx = sorted(yx, key = lambda t: t[0])\n",
    "    newPredY = [x for y, x in yx]\n",
    "    predYEntroList = newPredY[:int(len(newPredY)*0.5)]\n",
    "    predY = np.mean(predYEntroList, axis=0)\n",
    "    return predY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(labels_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for key, auth in labels_index.iteritems():\n",
    "    if auth == author_id:\n",
    "        loc = key\n",
    "        \n",
    "ans = predY[loc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(predY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
