{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# coding: utf-8\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "np.random.seed(123)\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "from sklearn.pipeline import Pipeline \n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "BASE_DIR = '../../'\n",
    "GLOVE_DIR = BASE_DIR + 'glove/'\n",
    "\n",
    "\n",
    "svc_tfidf = Pipeline([\n",
    "    (\"tfidf_vectorizer\", TfidfVectorizer(analyzer=lambda x: x)), \n",
    "    (\"linear svc\", SVC(kernel=\"linear\"))\n",
    "])\n",
    "doc_id = 161\n",
    "author_id = 80\n",
    "authorList = [11, 18, 80, 88, 64]\n",
    "chunk_size = 1500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "texts = []  # list of text samples\n",
    "labels_index = {}  # dictionary mapping label name to numeric id\n",
    "labels = []  # list of label ids\n",
    "import DatabaseQuery\n",
    "# textToUse = pd.read_csv(\"suffle_4_6000.csv\", names=[\"author_id\", \"doc_content\"], dtype={'author_id': int})\n",
    "from sshtunnel import SSHTunnelForwarder\n",
    "PORT=5432\n",
    "with SSHTunnelForwarder(('srn02.cs.cityu.edu.hk', 22),\n",
    "                        ssh_username='stylometry',\n",
    "                        ssh_password='stylometry',\n",
    "                        remote_bind_address=('localhost', 5432),\n",
    "                        local_bind_address=('localhost', 5400)):\n",
    "    textToUse = DatabaseQuery.getWordAuthData(5400, authors = authorList, doc = doc_id,\n",
    "                                              chunk_size = chunk_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "textToUse.loc[1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labels = []\n",
    "texts = []\n",
    "size = []\n",
    "authorList = textToUse.author_id.unique()\n",
    "for auth in authorList:\n",
    "    current = textToUse.loc[textToUse['author_id'] == auth]\n",
    "    size.append(current.shape[0])\n",
    "    print(\"Author: %5s  Size: %5s\" % (auth, current.shape[0]))\n",
    "print(\"Min: %s\" % (min(size)))\n",
    "print(\"Max: %s\" % (max(size)))\n",
    "\n",
    "authorList = authorList.tolist()\n",
    "\n",
    "for auth in authorList:\n",
    "    current = textToUse.loc[textToUse['author_id'] == auth]\n",
    "    samples = min(size)\n",
    "    current = current.sample(n = samples)\n",
    "    textlist = current.doc_content.tolist()\n",
    "    texts = texts + textlist\n",
    "    labels = labels + [authorList.index(author_id) for author_id in current.author_id.tolist()]\n",
    "\n",
    "labels_index = {}\n",
    "labels_index[0] = 0\n",
    "for i, auth in enumerate(authorList):\n",
    "    labels_index[i] = auth\n",
    "\n",
    "del textToUse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# split the data into a training set and a validation set\n",
    "# from sklearn.model_selection import KFold\n",
    "# kfold = KFold(n_splits=6, shuffle=True, random_state=123)\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_val, y_train, y_val = train_test_split(texts, labels, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "svc_tfidf.fit(x_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Train Accuracy %s.' % (str(svc_tfidf.score(x_train, y_train))))\n",
    "print('Test Accuracy %s.' % (str(svc_tfidf.score(x_val, y_val))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "texts = []  # list of text samples\n",
    "labels = []  # list of label ids\n",
    "import DatabaseQuery\n",
    "# textToUse = pd.read_csv(\"suffle_4_6000.csv\", names=[\"author_id\", \"doc_content\"], dtype={'author_id': int})\n",
    "from sshtunnel import SSHTunnelForwarder\n",
    "PORT=5432\n",
    "with SSHTunnelForwarder(('srn02.cs.cityu.edu.hk', 22),\n",
    "                        ssh_username='stylometry',\n",
    "                        ssh_password='stylometry',\n",
    "                        remote_bind_address=('localhost', 5432),\n",
    "                        local_bind_address=('localhost', 5400)):\n",
    "    textToUse = DatabaseQuery.getWordDocData(5400, doc_id, chunk_size = chunk_size)\n",
    "labels = []\n",
    "texts = []\n",
    "for index, row in textToUse.iterrows():\n",
    "    labels.append(authorList.index(row.author_id))\n",
    "    texts.append(row.doc_content)\n",
    "        \n",
    "print('Found %s texts.' % len(texts))\n",
    "\n",
    "del textToUse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "svc_tfidf.score(texts, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predList = svc_tfidf.predict(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predY = []\n",
    "for i, val in enumerate(authorList):\n",
    "    pred = (float)(list(predList).count(i)) / len(predList)\n",
    "    predY.append(pred * )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i, val in enumerate(predY):\n",
    "    print(\"%s  %s\" % (str(i), str(val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labels_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
